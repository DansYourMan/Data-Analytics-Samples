{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSE 532 Assignment 4 (Due 4/27/24)\n",
    "\n",
    "**Note: As with the previous assignment you should submit a separate document (.pdf or .doc(x)) with your responses to the analysis portion of the problems.** \n",
    "\n",
    "**1. (Machine Learning (Classification))** <br>a. Choose one of the [toy classification datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) bundled with sklearn **other than the digits dataset**. <br> b. Train **three** distinct sklearn classification estimators for the chosen dataset and compare the results to see which one performs the best when using **2-fold cross-validation**.  Note that you should use three distinct classification models here (not just tweak underlying parameters).  A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use classifiers!  Unless you have an inclination to do otherwise, I recommend using the model default parameters when available.   <br> c. Repeat a. for **20-fold cross-validation**. Explain in a paragraph the difference in your results when using 20-fold vs 2-fold cross-validation (if any). <br>d. Construct a **confusion matrix** for your _most accurate_ model between the three estimators and two cross-fold options. <br> e. Which class in your dataset is most accurately predicted to have the correct label by the best classifier, and and which is most likely to be confused among one or more of the wrong classes?_(You can use a cell in a jupyter notebook file for this or a separate text/document file)._\n",
    "\n",
    "**2 (Option I). (Machine Learning (Regression))** <br>a. Locate a non-proprietary, small-scale dataset _suitable for regression_ online.  There are countless sources and repositories than you can use in this task, but if you have trouble finding one, I recommend starting via Kaggle (https://www.kaggle.com/code/rtatman/datasets-for-regression-analysis/notebook).  Explain briefly what the dataset represents, what target variable you will be using, and what other features are present.  _You may want or need to apply preprocessing to your data to insure it can be used properly with the regression models_ (e.g. making every feature numeric through transformation or by dropping some)  <br> b. Train **three** distinct sklearn regression estimators for the chosen dataset and compare the results to see which one performs the best when using **10-fold cross-validation**, utilizing the R-Squared score to gauge performance.  Note that you should use two distinct regression models here (not just tweak underlying parameters).  A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use regression models!  Unless you have an inclination to do otherwise, I recommend using the model default parameters when available.<br>  c. Repeat part b utilizing the Mean Square Error to gauge performance.  _Briefly_ research the difference between the two metrics (MSE and R2), and explain in a paragraph or two i. the difference between them ii. when each one is the preferable metric to use. _(You can use a cell in a jupyter notebook file for this or a separate text/document file)._\n",
    "\n",
    "**2 (Option II). (PRAW and Sentiment)** <br>*Note: you should feel free to propose an alternative task using PRAW of your own design to me via email if you have a specific one in mind.*\n",
    "<br>a. Use PRAW to extract the 20 top submissions of all time from each of five related subreddits of your choice (ex: someone interested in sports subreddits might extract the top 20 posts from r/basketball, top 20 posts from r/football, etc.). \n",
    "<br>b. Use a sentiment analyzer (via Textblob or one of your choosing) to determine the positive sentiment of each and every top submission (all 100 in total), and store these in a variable of your choosing (data-frame, list of lists, etc.)  \n",
    "c. Investigate how to use a Python [box-plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html).  After doing so, produce a box-plot (default parameters are fine) for the sentiment measures of each of your five subreddits.  Your presentation should look _something_ like the image at the bottom of this file, with your five chosen subreddits replacing the x-axis labels (Subreddit 1 .. Subreddit 5).\n",
    "<br>d. Repeat steps a-c but use the 20 most controversial submissions of all time for each of your five subreddits.<br>e. Does anything surprise you about the distribution of sentiments, either with respect to individual subreddits, differences between the five subreddits, or the differences between _top_ and _controversial_ submissions?  Explain your answer in a few sentences.  _(You can use a cell in a jupyter notebook file for this or a separate text/document file)._\n",
    "<br>![Box Plot Example](A4BPExample.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors:\n",
      "  Average Accuracy: 0.95\n",
      "  Confusion Matrix:\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  5 45]]\n",
      "\n",
      "SVC:\n",
      "  Average Accuracy: 0.97\n",
      "  Confusion Matrix:\n",
      "[[50  0  0]\n",
      " [ 0 48  2]\n",
      " [ 0  2 48]]\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "  Average Accuracy: 0.97\n",
      "  Confusion Matrix:\n",
      "[[50  0  0]\n",
      " [ 0 48  2]\n",
      " [ 0  3 47]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the 2-fold cross-validation\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the three classifiers\n",
    "classifiers = {\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'Gaussian Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Perform 2-fold cross-validation for each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    # Calculate the accuracy scores using cross-validation\n",
    "    scores = cross_val_score(clf, X, y, cv=kf)\n",
    "    avg_accuracy = np.mean(scores)\n",
    "    \n",
    "    # Calculate predictions using cross-validation\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=kf)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    results[name] = {\n",
    "        'Average Accuracy': avg_accuracy,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Average Accuracy: {metrics['Average Accuracy']:.2f}\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(metrics['Confusion Matrix'])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors:\n",
      "  Average Accuracy: 0.96\n",
      "  Confusion Matrix:\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  2 48]]\n",
      "\n",
      "SVC:\n",
      "  Average Accuracy: 0.97\n",
      "  Confusion Matrix:\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  2 48]]\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "  Average Accuracy: 0.96\n",
      "  Confusion Matrix:\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the 2-fold cross-validation\n",
    "kf = KFold(n_splits=20, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the three classifiers\n",
    "classifiers = {\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'Gaussian Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Perform 2-fold cross-validation for each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    # Calculate the accuracy scores using cross-validation\n",
    "    scores = cross_val_score(clf, X, y, cv=kf)\n",
    "    avg_accuracy = np.mean(scores)\n",
    "    \n",
    "    # Calculate predictions using cross-validation\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=kf)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    results[name] = {\n",
    "        'Average Accuracy': avg_accuracy,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Average Accuracy: {metrics['Average Accuracy']:.2f}\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(metrics['Confusion Matrix'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1\n",
    "    There appears to be no difference between the 2 and 20 fold cross validation, as across all three models used there is a 0-1% accuracy difference accreditable to random chance. The confusion matrix tells similar story.\n",
    "    The first class has a 100% accuracy across all models. From the documentation that correlates to the I. Setosa plant. Both I. Versicolor and I. Virginica have 2-3 misclassification. SVC and Gaussian Naive Bayes have almost identical performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2, Option 1 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "  Average R-Squared: 0.60\n",
      "  Average MSE: 0.53\n",
      "\n",
      "Decision Tree Regressor:\n",
      "  Average R-Squared: 0.61\n",
      "  Average MSE: 0.51\n",
      "\n",
      "Random Forest Regressor:\n",
      "  Average R-Squared: 0.81\n",
      "  Average MSE: 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the three regression estimators\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "    'Random Forest Regressor': RandomForestRegressor()\n",
    "}\n",
    "\n",
    "# Perform 10-fold cross-validation for each estimator\n",
    "# Calculate both R-Squared and MSE\n",
    "results = {}\n",
    "for name, reg in regressors.items():\n",
    "    # Calculate R-Squared scores using cross-validation\n",
    "    r2_scores = cross_val_score(reg, X, y, cv=kf, scoring='r2')\n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    \n",
    "    # Calculate MSE scores using cross-validation\n",
    "    mse_scores = cross_val_score(reg, X, y, cv=kf, scoring=make_scorer(mean_squared_error))\n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    \n",
    "    # Store the average R-Squared and MSE in the dictionary\n",
    "    results[name] = {\n",
    "        'Average R-Squared': avg_r2,\n",
    "        'Average MSE': avg_mse\n",
    "    }\n",
    "\n",
    "# Display the results for each estimator\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Average R-Squared: {metrics['Average R-Squared']:.2f}\")\n",
    "    print(f\"  Average MSE: {metrics['Average MSE']:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2:\n",
    "\n",
    "\tAfter some basic research I discovered the sklearn California housing dataset. This dataset has over 20,000 samples, significantly above the 150 toy classification dataset. The dataset contains several attributes of a California district. The target variable is the median house value in California districts.\n",
    "    \n",
    "Features Present-\n",
    "\n",
    "    • Median income in the district\n",
    "\n",
    "    • Average house age in the district\n",
    "\n",
    "    • Average number of rooms per house\n",
    "\n",
    "    • Average number of bedrooms per house\n",
    "\n",
    "    • Population in the district\n",
    "\n",
    "    • Number of households in the district\n",
    "\n",
    "    • Latitude of the district\n",
    "\n",
    "    • Longitude of the district\n",
    "\n",
    "\tR-Squared is a relative measure of fit, indicating how well the model explains the variance in the target variable, while MSE is an absolute measure of fit, assessing the average squared deviation between actual and predicted values.\n",
    "    \n",
    "\tR-Squared is preferred when you want to understand the proportion of variance explained by the model.\n",
    "\tMSE is preferred when you want to measure the actual magnitude of error and assess the quality of predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
